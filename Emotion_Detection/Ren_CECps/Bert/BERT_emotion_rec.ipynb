{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "39e50228-a260-423d-baa9-0ff0081c8861",
   "metadata": {},
   "source": [
    "### 1. Import the training data and validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6f083fc6-b5ed-4cf1-b2e4-dabb6f5aa3e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train and validation data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "train_df = pd.read_csv('./train.tsv', sep='\\t', encoding='utf_8')\n",
    "val_df = pd.read_csv('./val.tsv', sep='\\t', encoding='utf_8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "580d7931-1102-46b9-8c94-0b8c005fee4b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>在一些大城市禁放烟花爆竹，是政府的一项利国利民的英明决策，只可惜好景不长。</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>“女性朋友”，又见官员的“女性朋友”。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>不想他正好在乡下，约好了和我相见，当我告诉他要借钱的事，他显得有些无奈。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>还好，现在开始下雨了。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>不过，在雇主与保姆“双选”这一过程中，一些保姆竟然先打听起雇主是从事啥职业的，看对自己家人有...</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           Sentences  label\n",
       "0              在一些大城市禁放烟花爆竹，是政府的一项利国利民的英明决策，只可惜好景不长。      1\n",
       "1                                “女性朋友”，又见官员的“女性朋友”。      2\n",
       "2               不想他正好在乡下，约好了和我相见，当我告诉他要借钱的事，他显得有些无奈。      2\n",
       "3                                        还好，现在开始下雨了。      0\n",
       "4  不过，在雇主与保姆“双选”这一过程中，一些保姆竟然先打听起雇主是从事啥职业的，看对自己家人有...      2"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "167fdf9b-6c44-4ee8-a715-853cd60e7531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Sentences</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>“老师不会告诉别人的，这是我们的秘密哦。</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>当理发女孩迟疑的眼眸对着我时，颤抖的嘴唇断续的音：真的要剪掉吗？</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>在这个历史的节点上，传统中国与现代中国，中国与西方，天下与世界，贯通为一。</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>现在，我需要的是把自己最真的一面展现出来，而不是萎缩，再也不是！</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>小王老师就羞赧着说，麻烦也来了呢！</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               Sentences  label\n",
       "0                   “老师不会告诉别人的，这是我们的秘密哦。      2\n",
       "1       当理发女孩迟疑的眼眸对着我时，颤抖的嘴唇断续的音：真的要剪掉吗？      5\n",
       "2  在这个历史的节点上，传统中国与现代中国，中国与西方，天下与世界，贯通为一。      0\n",
       "3       现在，我需要的是把自己最真的一面展现出来，而不是萎缩，再也不是！      7\n",
       "4                      小王老师就羞赧着说，麻烦也来了呢！      0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e0940ccb-08eb-4366-90f7-543eea2a578d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'Happy', 1: 'Sad', 2: 'Fear', 3: 'Surprise', 4: 'Anger', 5: 'Neutral'}\n",
      "2    4322\n",
      "1    3546\n",
      "0    2178\n",
      "5    1609\n",
      "4     748\n",
      "3     354\n",
      "Name: label, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "emotion_dict = {0: 'Happy', 1: 'Sad', 2: 'Fear', 3: 'Surprise', 4: 'Anger',\n",
    "                5: 'Neutral'}\n",
    "print(emotion_dict)\n",
    "print(train_df['label'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "483caec7-0e8e-4902-962f-b4bc5b1c823c",
   "metadata": {},
   "source": [
    "### 2. Create Dataset and Dataloader for model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9dc5b272-653a-43ca-b391-45c9984c1c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作一個可以用來讀取訓練 / 測試集的 Dataset，這是你需要徹底了解的部分。\n",
    "此 Dataset 每次將 tsv 裡的一筆成對句子轉換成 Albert 相容的格式，並回傳 3 個 tensors：\n",
    "- tokens_tensor：兩個句子合併後的索引序列，包含 [CLS] 與 [SEP]\n",
    "- segments_tensor：可以用來識別兩個句子界限的 binary tensor\n",
    "- label_tensor：將分類標籤轉換成類別索引的 tensor, 如果是測試集則回傳 None\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "class EmotionDataset(Dataset):\n",
    "    # 讀取前處理後的 tsv 檔並初始化一些參數\n",
    "    def __init__(self, mode, tokenizer):\n",
    "        assert mode in ['train', 'val' ,'test']\n",
    "        self.mode = mode\n",
    "        # 大數據你會需要用 iterator=True\n",
    "        self.df = pd.read_csv(mode + '.tsv', sep='\\t').fillna(\"\")\n",
    "        self.len = len(self.df)\n",
    "        # self.label_map = {'agreed': 0, 'disagreed': 1, 'unrelated': 2} ## not useful in this emotion df (label already numbers)\n",
    "        self.tokenizer = tokenizer\n",
    "    \n",
    "     # 定義回傳一筆訓練 / 測試數據的函式\n",
    "    # @pysnooper.snoop()\n",
    "    def __getitem__(self, idx):\n",
    "        sentence, label = self.df.iloc[idx, :].values\n",
    "        # 將 label 也轉換成索引方便轉換成 tensor\n",
    "        label_tensor = torch.tensor(label)\n",
    "        \n",
    "        # 建立第一個句子的 BERT tokens 並加入分隔符號 [SEP]\n",
    "        word_pieces = [\"[CLS]\"]\n",
    "        tokens_a = self.tokenizer.tokenize(sentence)\n",
    "        word_pieces += tokens_a + [\"[CLS]\"]\n",
    "        num_of_words = len(word_pieces)\n",
    "        \n",
    "        \n",
    "        # 將整個 token 序列轉換成索引序列\n",
    "        ids = self.tokenizer.convert_tokens_to_ids(word_pieces)\n",
    "        tokens_tensor = torch.tensor(ids)\n",
    "        \n",
    "        # 將第一句包含 [SEP] 的 token 位置設為 0，其他為 1 表示第二句\n",
    "        segments_tensor = torch.tensor([0] * num_of_words, \n",
    "                                        dtype=torch.long)\n",
    "        \n",
    "        return (tokens_tensor, segments_tensor, label_tensor)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8945767e-4483-489f-839e-1f3f5f82a7cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 1762,  671,  763, 1920, 1814, 2356, 4881, 3123, 4170, 5709, 4255,\n",
       "         5001, 8024, 3221, 3124, 2424, 4638,  671, 7555, 1164, 1744, 1164, 3696,\n",
       "         4638, 5739, 3209, 1104, 5032, 8024, 1372, 1377, 2667, 1962, 3250,  679,\n",
       "         7270,  511,  101]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(1))"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "pretrained_model_name = \"bert-base-chinese\"\n",
    "# define BERT tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained(pretrained_model_name)\n",
    "\n",
    "# create training set\n",
    "trainset = EmotionDataset(\"train\", tokenizer=tokenizer)\n",
    "trainset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2513ac52-bca6-4658-8991-d432668bb180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 101, 3189, 3315, 3300, 6387, 1914, 4035, 4514, 1416, 8024, 6006, 6432,\n",
       "         6929, 4035, 4514,  758, 5709, 1061, 7305, 8024, 5273, 5273, 5344, 5344,\n",
       "         8024, 4692,  677, 1343, 1282, 1146, 4178, 7317, 8024,  852, 3796, 4035,\n",
       "         4514, 1416, 4638,  782, 8024, 3187, 6389, 3221, 1920,  782, 6820, 3221,\n",
       "         2207, 2111, 8024, 1316, 1139, 1936, 1765, 2128, 7474, 8024, 6432, 6929,\n",
       "         3221, 1369, 5102,  741, 2421,  738, 6387, 3291, 1394, 6844,  511,  101]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
       " tensor(3))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create validation set\n",
    "valset = EmotionDataset(\"val\", tokenizer=tokenizer)\n",
    "valset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3dbaa07a-9423-4925-a4b9-f65d8301136e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "實作可以一次回傳一個 mini-batch 的 DataLoader\n",
    "這個 DataLoader 吃我們上面定義的 `FakeNewsDataset`，\n",
    "回傳訓練 BERT 時會需要的 4 個 tensors：\n",
    "- tokens_tensors  : (batch_size, max_seq_len_in_batch)\n",
    "- segments_tensors: (batch_size, max_seq_len_in_batch)\n",
    "- masks_tensors   : (batch_size, max_seq_len_in_batch)\n",
    "- label_ids       : (batch_size)\n",
    "\"\"\"\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "# 這個函式的輸入 `samples` 是一個 list，裡頭的每個 element 都是\n",
    "# 剛剛定義的 `FakeNewsDataset` 回傳的一個樣本，每個樣本都包含 3 tensors：\n",
    "# - tokens_tensor\n",
    "# - segments_tensor\n",
    "# - label_tensor\n",
    "# 它會對前兩個 tensors 作 zero padding，並產生前面說明過的 masks_tensors\n",
    "def create_mini_batch(samples):\n",
    "    tokens_tensors = [s[0] for s in samples]\n",
    "    segments_tensors = [s[1] for s in samples]\n",
    "    \n",
    "    # 測試集有 labels\n",
    "    if samples[0][2] is not None:\n",
    "        label_ids = torch.stack([s[2] for s in samples])\n",
    "    else:\n",
    "        label_ids = None\n",
    "    \n",
    "    # zero pad 到同一序列長度\n",
    "    tokens_tensors = pad_sequence(tokens_tensors, \n",
    "                                  batch_first=True)\n",
    "    segments_tensors = pad_sequence(segments_tensors, \n",
    "                                    batch_first=True)\n",
    "    \n",
    "    # attention masks，將 tokens_tensors 裡頭不為 zero padding\n",
    "    # 的位置設為 1 讓 BERT 只關注這些位置的 tokens\n",
    "    masks_tensors = torch.zeros(tokens_tensors.shape, \n",
    "                                dtype=torch.long)\n",
    "    masks_tensors = masks_tensors.masked_fill(tokens_tensors != 0, 1)\n",
    "    \n",
    "    return tokens_tensors, segments_tensors, masks_tensors, label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f831ce86-5c27-422e-98c9-9d0d62120cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化一個每次回傳 64 個訓練樣本的 DataLoader\n",
    "# 利用 `collate_fn` 將 list of samples 合併成一個 mini-batch 是關鍵\n",
    "\n",
    "# create dataloader for training data\n",
    "BATCH_SIZE = 16\n",
    "trainloader = DataLoader(trainset, shuffle=True, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5aade772-992e-4e75-8e95-ae616604493d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create dataloader for validation data\n",
    "BATCH_SIZE = 16\n",
    "valloader = DataLoader(valset, batch_size=BATCH_SIZE, \n",
    "                         collate_fn=create_mini_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f7e28e97-4047-4678-b90c-9389863fd2ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tokens_tensors.shape   = torch.Size([16, 54]) \n",
      "tensor([[ 101, 2769,  679, 4761, 6887, 6821, 3221, 1962, 6820, 3221,  679, 1962,\n",
      "          511,  101,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 4197, 5445, 8024,  800,  812, 3766, 3300, 5632, 2346, 4638, 1164,\n",
      "         4660,  807, 6134, 8024, 3300, 1525,  671,  702,  782, 1920,  807, 6134,\n",
      "         3221, 4696, 3633,  807, 6134,  749, 6821,  763, 1814, 2356, 4683, 3837,\n",
      "         4638, 4495, 2100, 3326, 1164, 1469, 1164, 4660, 6206, 3724, 1450, 8043,\n",
      "          101,    0,    0,    0,    0,    0],\n",
      "        [ 101,  791, 1921, 1962, 1008, 7439,  678,  676, 2428, 1416,  511,  101,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2769,  812, 6963, 1762, 4685,  849, 4638, 6758, 6887,  677, 1184,\n",
      "         6626, 1400, 5326,  511,  101,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1126,  725, 3766, 6432, 6814, 1126, 1368, 6413, 8024, 7370,  749,\n",
      "         7309, 6662,  511,  101,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 4385, 1762, 6662,  738,  934, 1962,  749, 8024,  794, 6205, 2123,\n",
      "         1168, 4373, 3409, 6628,  671, 1921, 2218, 1377,  809, 1168, 6809, 8024,\n",
      "         6929, 3198,  952, 6206, 1762,  704, 6854,  977, 7561,  511,  101,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1377, 3221, 1343, 2399, 1100, 2108, 4638,  671, 1921, 8024,  680,\n",
      "         1988, 2094,  794, 1555, 1767, 1726, 3341, 2798, 1355, 4385, 6821,  816,\n",
      "         6132, 3302, 1400, 7481, 2419, 6804,  677, 6158, 7229, 1690, 1153, 1139,\n",
      "          749,  697,  702,  671, 2189, 1914, 7270, 4638, 1366, 2094,  511,  101,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 5303,  754, 1355, 4385, 8024, 5314, 5632, 2346, 2823, 4638,  955,\n",
      "         1366, 3221, 6929,  720,  679, 1838,  671, 1140,  511,  101,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  852, 1315, 3221, 1599,  752, 8024, 2769,  812,  738, 3221, 6474,\n",
      "         2708, 4638, 2476, 2813,  511,  101,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1728,  711, 3766, 3300,  741,  511,  101,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  671,  833, 2797, 3322, 7190, 1898, 1510, 6629, 8024, 2769, 4761,\n",
      "         6887,  671, 2137, 3221, 3301, 1351, 4638, 4867, 4886, 4764,  928, 8024,\n",
      "         1506, 8024, 1069, 1939, 4007, 1921, 7607,  511,  101,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 2769,  812, 7444, 6206, 1726, 2495, 4638, 3152, 4289, 6820, 3300,\n",
      "         2523, 1914, 8024, 1963, 3362, 6821, 3613, 4696, 4638, 5709,  749, 8030,\n",
      "          783, 4638, 1921,  817,  743, 1726, 1749, 3209, 1736, 1077, 7674, 7198,\n",
      "         1008, 8024, 6929, 1071,  800, 4638, 3152, 4289, 2769,  812, 6820, 5543,\n",
      "          743, 2533, 6629, 1408, 8043,  101],\n",
      "        [ 101,  852, 1127, 4495, 1462, 4638, 4685, 6878, 1469, 3092, 5504, 8024,\n",
      "         4639, 3221, 1963, 3634, 1975,  679, 1377, 6241,  511,  101,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101,  791, 1921, 8024, 3221,  702, 5279, 2573, 3189,  100,  100, 2769,\n",
      "          812, 4685, 6399, 4638, 5018,  758,  702, 2399, 1928, 8013,  101,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 1963, 3362, 2207, 6432, 6574, 7030,  679, 1962, 8024, 5041, 5276,\n",
      "          722, 1400, 1353, 5445, 3221,  702, 4958, 1900, 8024, 3221,  702, 5168,\n",
      "         6610,  511,  101,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0],\n",
      "        [ 101, 6006, 4197,  800, 1440, 6401, 2769, 8024,  872, 2190, 2769,  812,\n",
      "         4638, 2697, 2658, 3193, 2347, 5307, 3909,  749, 8024,  738, 1728,  711,\n",
      "         2769, 3300,  763,  752, 6375,  872, 2190, 2769, 1927, 1343,  749, 4263,\n",
      "          511,  101,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0]])\n",
      "------------------------\n",
      "segments_tensors.shape = torch.Size([16, 54])\n",
      "tensor([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "masks_tensors.shape    = torch.Size([16, 54])\n",
      "tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
      "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0],\n",
      "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "         0, 0, 0, 0, 0, 0]])\n",
      "------------------------\n",
      "label_ids.shape        = torch.Size([16])\n",
      "tensor([2, 1, 2, 1, 1, 0, 2, 1, 2, 2, 0, 2, 3, 0, 2, 1])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data = next(iter(trainloader))\n",
    "\n",
    "tokens_tensors, segments_tensors, \\\n",
    "    masks_tensors, label_ids = data\n",
    "\n",
    "print(f\"\"\"\n",
    "tokens_tensors.shape   = {tokens_tensors.shape} \n",
    "{tokens_tensors}\n",
    "------------------------\n",
    "segments_tensors.shape = {segments_tensors.shape}\n",
    "{segments_tensors}\n",
    "------------------------\n",
    "masks_tensors.shape    = {masks_tensors.shape}\n",
    "{masks_tensors}\n",
    "------------------------\n",
    "label_ids.shape        = {label_ids.shape}\n",
    "{label_ids}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4316716-ddc8-4fa8-84a2-9136cda7b983",
   "metadata": {
    "tags": []
   },
   "source": [
    "### 3. Add a new layer on top of BERT to complete downstream task\n",
    "\n",
    "- Via BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c25b3e-e934-4ef5-9e3f-d6278268be2c",
   "metadata": {},
   "source": [
    "##### Model 1: Using Default BertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "fc41de1a-b1e2-4e24-885b-6951ede445fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.1, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=6, bias=True)\n"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from transformers import BertForSequenceClassification\n",
    "\n",
    "pretrained_model_name = \"bert-base-chinese\"\n",
    "num_labels = 6\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "                pretrained_model_name, num_labels=num_labels)\n",
    "\n",
    "clear_output()\n",
    "\n",
    "# high-level 顯示此模型裡的 modules\n",
    "print(\"\"\"\n",
    "name            module\n",
    "----------------------\"\"\")\n",
    "for name, module in model.named_children():\n",
    "    if name == \"bert\":\n",
    "        for n, _ in module.named_children():\n",
    "            print(f\"{name}:{n}\")\n",
    "    else:\n",
    "        print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f15ab1-f690-417a-a704-119102f71b8d",
   "metadata": {},
   "source": [
    "##### Model 2: Increase the Dropout rate (to 0.2) for both fully connected layers and the attention probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8a0801d5-48d6-4669-a844-4670fa51418c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "name            module\n",
      "----------------------\n",
      "bert:embeddings\n",
      "bert:encoder\n",
      "bert:pooler\n",
      "dropout         Dropout(p=0.2, inplace=False)\n",
      "classifier      Linear(in_features=768, out_features=9, bias=True)\n"
     ]
    }
   ],
   "source": [
    "# from IPython.display import clear_output\n",
    "# from transformers import BertForSequenceClassification, BertModel\n",
    "\n",
    "# pretrained_model_name = \"bert-base-chinese\"\n",
    "# num_labels = 9\n",
    "\n",
    "# model = BertForSequenceClassification.from_pretrained(\n",
    "#                 pretrained_model_name, num_labels=num_labels,\n",
    "#                 hidden_dropout_prob = 0.2,\n",
    "#                 attention_probs_dropout_prob = 0.2)\n",
    "\n",
    "# clear_output()\n",
    "\n",
    "# # high-level 顯示此模型裡的 modules\n",
    "# print(\"\"\"\n",
    "# name            module\n",
    "# ----------------------\"\"\")\n",
    "# for name, module in model.named_children():\n",
    "#     if name == \"bert\":\n",
    "#         for n, _ in module.named_children():\n",
    "#             print(f\"{name}:{n}\")\n",
    "#     else:\n",
    "#         print(\"{:15} {}\".format(name, module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "e26b3d4b-8aab-4178-b14e-a8d31f377485",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "整個分類模型的參數量：102272262\n",
      "線性分類器的參數量：4614\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 算算整個分類模型以及裡頭的簡單分類器有多少參數：\n",
    "def get_learnable_params(module):\n",
    "    return [p for p in module.parameters() if p.requires_grad]\n",
    "     \n",
    "model_params = get_learnable_params(model)\n",
    "clf_params = get_learnable_params(model.classifier)\n",
    "\n",
    "print(f\"\"\"\n",
    "整個分類模型的參數量：{sum(p.numel() for p in model_params)}\n",
    "線性分類器的參數量：{sum(p.numel() for p in clf_params)}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "ce23b34b-9213-4f90-b686-570b4e66a119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "classification acc: 0.10223116313094367\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "定義一個可以針對特定 DataLoader 取得模型預測結果以及分類準確度的函式\n",
    "之後也可以用來生成上傳到 Kaggle 競賽的預測結果\n",
    "\n",
    "2019/11/22 更新：在將 `tokens`、`segments_tensors` 等 tensors\n",
    "丟入模型時，強力建議指定每個 tensor 對應的參數名稱，以避免 HuggingFace\n",
    "更新 repo 程式碼並改變參數順序時影響到我們的結果。\n",
    "\"\"\"\n",
    "\n",
    "def get_predictions(model, dataloader, compute_acc=False):\n",
    "    predictions = None\n",
    "    correct = 0\n",
    "    total = 0\n",
    "      \n",
    "    with torch.no_grad():\n",
    "        # 遍巡整個資料集\n",
    "        for data in dataloader:\n",
    "            # 將所有 tensors 移到 GPU 上\n",
    "            if next(model.parameters()).is_cuda:\n",
    "                data = [t.to(\"cuda:0\") for t in data if t is not None]\n",
    "            \n",
    "            \n",
    "            # 別忘記前 3 個 tensors 分別為 tokens, segments 以及 masks\n",
    "            # 且強烈建議在將這些 tensors 丟入 `model` 時指定對應的參數名稱\n",
    "            tokens_tensors, segments_tensors, masks_tensors = data[:3]\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors)\n",
    "            \n",
    "            logits = outputs[0]\n",
    "            _, pred = torch.max(logits.data, 1)\n",
    "            \n",
    "            # 用來計算訓練集的分類準確率\n",
    "            if compute_acc:\n",
    "                labels = data[3]\n",
    "                total += labels.size(0)\n",
    "                correct += (pred == labels).sum().item()\n",
    "                \n",
    "            # 將當前 batch 記錄下來\n",
    "            if predictions is None:\n",
    "                predictions = pred\n",
    "            else:\n",
    "                predictions = torch.cat((predictions, pred))\n",
    "    \n",
    "    if compute_acc:\n",
    "        acc = correct / total\n",
    "        return predictions, acc\n",
    "    return predictions\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "_, acc = get_predictions(model, valloader, compute_acc=True)\n",
    "print(\"classification acc:\", acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "763ad029-cd25-4ec4-873b-d585ae269f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# default `log_dir` is \"runs\" - we'll be more specific here\n",
    "writer = SummaryWriter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "329dd990-9a4b-410f-9651-4aa0ee4ea851",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 24276), started 2:16:48 ago. (Use '!kill 24276' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-d23eafb220617737\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-d23eafb220617737\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir 'runs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "e28b5041-b6b1-4ee6-bed5-071e6a0e1924",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[epoch 1] training loss: 969.364, training acc: 0.668, validation acc: 0.591\n",
      "[epoch 2] training loss: 772.521, training acc: 0.740, validation acc: 0.577\n",
      "[epoch 3] training loss: 605.269, training acc: 0.817, validation acc: 0.597\n",
      "[epoch 4] training loss: 449.908, training acc: 0.878, validation acc: 0.582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<timed exec>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# 訓練模式\n",
    "device = \"cuda:0\"\n",
    "\n",
    "\n",
    "# 使用 Adam Optim 更新整個分類模型的參數\n",
    "learning_rate = 5e-5\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "EPOCHS = 8\n",
    "train_running_loss = 0.0\n",
    "val_running_loss = 0.0\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Training \n",
    "    model.train()\n",
    "    for i, data in enumerate(trainloader):\n",
    "        \n",
    "        tokens_tensors, segments_tensors, \\\n",
    "        masks_tensors, labels = [t.to(device) for t in data]\n",
    "\n",
    "        # 將參數梯度歸零\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(input_ids=tokens_tensors, \n",
    "                        token_type_ids=segments_tensors, \n",
    "                        attention_mask=masks_tensors, \n",
    "                        labels=labels)\n",
    "        loss = outputs[0]\n",
    "        # backward\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Record loss to tensorbard\n",
    "        train_running_loss += loss.item()\n",
    "        if i % 10 == 9:    # every 10 mini-batches...\n",
    "            # ...log the running loss\n",
    "            writer.add_scalar('training loss',\n",
    "                            loss.item() / 10,\n",
    "                            epoch * len(trainloader) + i)\n",
    "            \n",
    "    # Validation\n",
    "    model.train()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(valloader):\n",
    "            # forward pass\n",
    "            outputs = model(input_ids=tokens_tensors, \n",
    "                            token_type_ids=segments_tensors, \n",
    "                            attention_mask=masks_tensors, \n",
    "                            labels=labels)\n",
    "            loss = outputs[0]\n",
    "            val_running_loss += loss.item()\n",
    "        # ...log the running loss\n",
    "        writer.add_scalar('validation loss',\n",
    "                        val_running_loss/len(valloader),\n",
    "                        epoch + 1)\n",
    "    \n",
    "    \n",
    "    \n",
    "    # 計算分類準確率\n",
    "    _, train_acc = get_predictions(model, trainloader, compute_acc=True)\n",
    "    _, val_acc = get_predictions(model, valloader, compute_acc=True)\n",
    "\n",
    "    print('[epoch %d] training loss: %.3f, training acc: %.3f, validation acc: %.3f' %\n",
    "          (epoch + 1, train_running_loss, train_acc, val_acc))\n",
    "    \n",
    "    writer.add_scalar('training accuracy',\n",
    "                        train_acc,\n",
    "                        epoch + 1)\n",
    "    writer.add_scalar('validation accuracy',\n",
    "                        val_acc,\n",
    "                        epoch + 1)\n",
    "    \n",
    "    # save model for every epoch\n",
    "    save_directory = f'.\\saved_model\\Bert_3\\Epoch{epoch+1}'   ### Change this before training new model\n",
    "    tokenizer.save_pretrained(save_directory)\n",
    "    model.save_pretrained(save_directory)\n",
    "    \n",
    "    \n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0dcb1640-0cb7-40b6-8630-0481ea1b379e",
   "metadata": {},
   "outputs": [],
   "source": [
    "writer.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee81463e-bb1c-4a8d-93f4-38006986a2f7",
   "metadata": {},
   "source": [
    "### 3. Predict new input sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2edfb19a-2814-45a0-a15f-09ca82a62ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load trained model\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "model_path = 'D:/HKU/FYP/research_method/Emotion_Detection/Ren_CECps/Bert/saved_model/Bert_2/Epoch4'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "756dbf34-3bc9-43cf-bbae-c75c422bf9d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion_dict = {0: 'Happy', 1: 'Sad', 2: 'Fear', 3: 'Surprise', 4: 'Anger',\n",
    "                5: 'Neutral'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6050a5d7-289b-43ef-b8f3-a1ad659949d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda:0\n",
      "tensor([[9.8951e-01, 7.8748e-04, 1.1294e-03, 3.2023e-03, 1.6918e-03, 3.1902e-03,\n",
      "         1.2232e-04, 1.9207e-04, 1.7354e-04],\n",
      "        [2.5000e-02, 1.8427e-02, 8.6540e-01, 1.9107e-02, 1.8076e-02, 5.3879e-02,\n",
      "         3.1401e-05, 4.5647e-05, 3.4724e-05],\n",
      "        [2.6426e-03, 3.5786e-03, 7.8153e-03, 2.8660e-03, 9.8064e-01, 1.4792e-03,\n",
      "         2.6240e-04, 5.1924e-04, 1.9747e-04],\n",
      "        [7.4592e-02, 7.4522e-02, 6.5876e-01, 7.7423e-02, 1.0753e-01, 6.9381e-03,\n",
      "         4.8619e-05, 1.2719e-04, 6.1229e-05],\n",
      "        [1.0010e-02, 1.7802e-03, 7.2933e-03, 1.8169e-03, 7.0223e-04, 9.7823e-01,\n",
      "         3.7622e-05, 6.6074e-05, 6.3261e-05],\n",
      "        [7.1304e-03, 2.1038e-02, 2.8067e-02, 9.7130e-04, 1.0018e-03, 9.4128e-01,\n",
      "         2.2050e-04, 1.8149e-04, 1.1411e-04],\n",
      "        [2.0978e-02, 4.8590e-01, 4.7537e-01, 2.8740e-03, 1.0018e-02, 4.7629e-03,\n",
      "         3.3819e-05, 2.5958e-05, 3.8533e-05]], device='cuda:0')\n",
      "tensor([0, 2, 4, 2, 5, 5, 1], device='cuda:0')\n",
      "Wall time: 121 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "model = model.to(device)\n",
    "\n",
    "X_train_chinese = [\"我很高兴可以见到你\", \"你在干什么呀你\", \"我真的是憤怒了\", \"你真的是疯了\", \"我想了解机器学习的课程\", \"今天\", \"公开试成绩不如理想\"]\n",
    "batch = tokenizer(X_train_chinese, padding=True, truncation=True, max_length=512, return_tensors='pt')\n",
    "batch = batch.to(device)\n",
    "\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = model(**batch)\n",
    "    logits = outputs[0]\n",
    "    predictions = F.softmax(outputs.logits, dim=1)\n",
    "    _, label = torch.max(logits.data, 1)\n",
    "    print(predictions)\n",
    "    print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb89050a-9d9d-482c-85a2-26ac3d1e27b5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7 (pytorch)",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
